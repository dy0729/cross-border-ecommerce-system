import pandas as pd
import numpy as np
import random
import time
from datetime import datetime
import json

class SupplierCrawler:
    """ä¾›åº”å•†æ•°æ®çˆ¬è™«ç±» - æ¨¡æ‹Ÿä»å¤šä¸ªå¹³å°è·å–ä¾›åº”å•†æ•°æ®"""
    
    def __init__(self):
        self.platforms = {
            "1688": "https://www.1688.com",
            "é˜¿é‡Œå·´å·´": "https://www.alibaba.com", 
            "æ…§èªç½‘": "https://www.hc360.com",
            "ä¸­å›½åˆ¶é€ ç½‘": "https://www.made-in-china.com"
        }
        
        # æ¨¡æ‹ŸçœŸå®çš„ä¾›åº”å•†æ•°æ®æ¨¡æ¿
        self.real_suppliers_template = [
            {
                "platform": "1688",
                "name": "å¹¿å·å¸‚ç™½äº‘åŒºæ—¶å°šæœé¥°å‚",
                "category": "å¥³è£…",
                "location": "å¹¿ä¸œå¹¿å·",
                "years": 8,
                "rating": 4.7,
                "reviews": 15680,
                "min_order": 100,
                "capacity": 50000,
                "delivery_time": 7,
                "certifications": ["BSCI", "ISO9001"],
                "price_level": "ä¸­ä»·",
                "quality_level": "ä¼˜è´¨"
            },
            {
                "platform": "é˜¿é‡Œå·´å·´",
                "name": "æ·±åœ³å¸‚é¾™åæ–°åŒºä¼˜è´¨ç”µå­å‚",
                "category": "ç”µå­äº§å“", 
                "location": "å¹¿ä¸œæ·±åœ³",
                "years": 12,
                "rating": 4.8,
                "reviews": 28900,
                "min_order": 500,
                "capacity": 100000,
                "delivery_time": 10,
                "certifications": ["CE", "FCC", "RoHS"],
                "price_level": "ä½ä»·",
                "quality_level": "æ ‡å‡†"
            },
            {
                "platform": "1688",
                "name": "ä¹‰ä¹Œå¸‚å°å•†å“æ‰¹å‘ä¸­å¿ƒ",
                "category": "å®¶å±…ç”¨å“",
                "location": "æµ™æ±Ÿä¹‰ä¹Œ",
                "years": 15,
                "rating": 4.5,
                "reviews": 45200,
                "min_order": 200,
                "capacity": 80000,
                "delivery_time": 5,
                "certifications": ["ISO9001"],
                "price_level": "ä½ä»·",
                "quality_level": "æ ‡å‡†"
            }
        ]
    
    def simulate_crawl_1688(self, category, max_results=20):
        """æ¨¡æ‹Ÿä»1688çˆ¬å–ä¾›åº”å•†æ•°æ®"""
        print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–1688å¹³å° - {category}ç±»åˆ«ä¾›åº”å•†...")
        time.sleep(1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        
        suppliers = []
        base_names = [
            "å¹¿å·æ—¶å°š", "æ·±åœ³ä¼˜è´¨", "ä¸œèç²¾å“", "ä½›å±±å›½é™…", "ä¸­å±±åˆ¶é€ ",
            "ç æµ·è®¾è®¡", "æƒ å·çººç»‡", "æ±Ÿé—¨å‡ºå£", "æ±•å¤´é›†å›¢", "æ½®å·å·¥å‚"
        ]
        
        suffixes = ["æœ‰é™å…¬å¸", "åˆ¶é€ å‚", "è´¸æ˜“å…¬å¸", "å·¥å‚", "ä¼ä¸š", "é›†å›¢"]
        
        for i in range(max_results):
            name = f"{random.choice(base_names)}{category}{random.choice(suffixes)}"
            
            supplier = {
                "platform": "1688",
                "name": name,
                "category": category,
                "location": random.choice(["å¹¿ä¸œ", "æµ™æ±Ÿ", "æ±Ÿè‹", "ç¦å»º", "å±±ä¸œ"]),
                "years": random.randint(3, 20),
                "rating": round(random.uniform(4.0, 5.0), 1),
                "reviews": random.randint(1000, 50000),
                "min_order": random.randint(50, 1000),
                "capacity": random.randint(5000, 100000),
                "delivery_time": random.randint(5, 25),
                "certifications": random.sample(["ISO9001", "BSCI", "WRAP", "OEKO-TEX", "CE", "FCC"], 
                                              random.randint(1, 3)),
                "price_level": random.choice(["ä½ä»·", "ä¸­ä»·", "é«˜ä»·"]),
                "quality_level": random.choice(["æ ‡å‡†", "ä¼˜è´¨", "ç²¾å“"]),
                "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            suppliers.append(supplier)
        
        return suppliers
    
    def simulate_crawl_alibaba(self, category, max_results=15):
        """æ¨¡æ‹Ÿä»é˜¿é‡Œå·´å·´çˆ¬å–ä¾›åº”å•†æ•°æ®"""
        print(f"ğŸ•·ï¸ æ­£åœ¨çˆ¬å–é˜¿é‡Œå·´å·´å¹³å° - {category}ç±»åˆ«ä¾›åº”å•†...")
        time.sleep(1.5)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        
        suppliers = []
        regions = ["Guangdong", "Zhejiang", "Jiangsu", "Fujian", "Shandong"]
        
        for i in range(max_results):
            name = f"{category} Manufacturer Co., Ltd"
            
            supplier = {
                "platform": "é˜¿é‡Œå·´å·´",
                "name": name,
                "category": category,
                "location": random.choice(regions),
                "years": random.randint(5, 25),
                "rating": round(random.uniform(4.2, 5.0), 1),
                "reviews": random.randint(2000, 80000),
                "min_order": random.randint(100, 2000),
                "capacity": random.randint(10000, 200000),
                "delivery_time": random.randint(7, 30),
                "certifications": random.sample(["ISO9001", "BSCI", "WRAP", "CE", "FCC", "RoHS"], 
                                              random.randint(2, 4)),
                "price_level": random.choice(["ä½ä»·", "ä¸­ä»·", "é«˜ä»·"]),
                "quality_level": random.choice(["æ ‡å‡†", "ä¼˜è´¨", "ç²¾å“"]),
                "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "export_experience": True,
                "trade_assurance": True
            }
            suppliers.append(supplier)
        
        return suppliers
    
    def crawl_suppliers_by_category(self, category, total_results=50):
        """æŒ‰ç±»åˆ«çˆ¬å–ä¾›åº”å•†æ•°æ®"""
        all_suppliers = []
        
        # ä»ä¸åŒå¹³å°çˆ¬å–æ•°æ®
        suppliers_1688 = self.simulate_crawl_1688(category, max_results=total_results//2)
        suppliers_alibaba = self.simulate_crawl_alibaba(category, max_results=total_results//2)
        
        all_suppliers.extend(suppliers_1688)
        all_suppliers.extend(suppliers_alibaba)
        
        # æ·»åŠ ä¸€äº›çœŸå®æ¨¡æ¿æ•°æ®
        for template in self.real_suppliers_template:
            if template["category"] == category:
                all_suppliers.append(template.copy())
        
        return all_suppliers
    
    def crawl_all_categories(self):
        """çˆ¬å–æ‰€æœ‰ç±»åˆ«çš„ä¾›åº”å•†æ•°æ®"""
        categories = ["å¥³è£…", "ç”·è£…", "ç«¥è£…", "ç”µå­äº§å“", "ç¾å¦†ç”¨å“", "å®¶å±…ç”¨å“"]
        all_data = []
        
        print("ğŸš€ å¼€å§‹å…¨é¢çˆ¬å–ä¾›åº”å•†æ•°æ®...")
        
        for category in categories:
            print(f"\nğŸ“‚ å¤„ç†ç±»åˆ«: {category}")
            suppliers = self.crawl_suppliers_by_category(category, total_results=30)
            all_data.extend(suppliers)
            
            # æ¨¡æ‹Ÿçˆ¬å–é—´éš”
            time.sleep(0.5)
        
        return all_data
    
    def save_to_dataframe(self, suppliers_data):
        """å°†çˆ¬å–çš„æ•°æ®è½¬æ¢ä¸ºDataFrameæ ¼å¼"""
        processed_data = []
        
        for supplier in suppliers_data:
            processed_supplier = {
                'åº—é“ºåç§°': supplier['name'],
                'å¹³å°æ¥æº': supplier['platform'],
                'åº—é“ºå¹´ä»½': f"{supplier['years']}å¹´",
                'åº—é“ºè¯„åˆ†': supplier['rating'],
                'åº—é“ºè¯„è®ºæ•°é‡': f"{supplier['reviews']:,}",
                'ä¸»è¥äº§å“': supplier['category'],
                'æœˆäº§èƒ½': f"{supplier['capacity']}ä»¶",
                'æœ€å°èµ·è®¢é‡': f"{supplier['min_order']}ä»¶",
                'äº¤è´§å‘¨æœŸ': f"{supplier['delivery_time']}å¤©",
                'æ‰€åœ¨åœ°åŒº': supplier['location'],
                'è®¤è¯æƒ…å†µ': ', '.join(supplier['certifications']) if supplier['certifications'] else 'æ— è®¤è¯',
                'ä»·æ ¼ç­‰çº§': supplier['price_level'],
                'è´¨é‡ç­‰çº§': supplier['quality_level'],
                'çˆ¬å–æ—¶é—´': supplier.get('crawl_time', datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
                'åˆä½œå¹´é™': f"{random.randint(1, supplier['years'])}å¹´",
                'é€€è´§ç‡': f"{round(random.uniform(0.5, 5.0), 1)}%",
                'å‡†æ—¶äº¤è´§ç‡': f"{round(random.uniform(85, 99), 1)}%"
            }
            
            # æ·»åŠ ç‰¹æ®Šå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'export_experience' in supplier:
                processed_supplier['å‡ºå£ç»éªŒ'] = 'æ˜¯' if supplier['export_experience'] else 'å¦'
            if 'trade_assurance' in supplier:
                processed_supplier['è´¸æ˜“ä¿éšœ'] = 'æ˜¯' if supplier['trade_assurance'] else 'å¦'
            
            processed_data.append(processed_supplier)
        
        return pd.DataFrame(processed_data)
    
    def update_supplier_database(self, output_file='è·¨å¢ƒç”µå•†/data/crawled_suppliers.csv'):
        """æ›´æ–°ä¾›åº”å•†æ•°æ®åº“"""
        print("ğŸ”„ å¼€å§‹æ›´æ–°ä¾›åº”å•†æ•°æ®åº“...")
        
        # çˆ¬å–æœ€æ–°æ•°æ®
        suppliers_data = self.crawl_all_categories()
        
        # è½¬æ¢ä¸ºDataFrame
        df = self.save_to_dataframe(suppliers_data)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        df.to_csv(output_file, index=False, encoding='utf-8')
        
        print(f"âœ… æˆåŠŸçˆ¬å–å¹¶ä¿å­˜äº† {len(df)} æ¡ä¾›åº”å•†æ•°æ®åˆ° {output_file}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\nğŸ“Š çˆ¬å–æ•°æ®ç»Ÿè®¡:")
        print(f"æ€»ä¾›åº”å•†æ•°é‡: {len(df)}")
        print(f"å¹³å°åˆ†å¸ƒ: {df['å¹³å°æ¥æº'].value_counts().to_dict()}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {df['ä¸»è¥äº§å“'].value_counts().to_dict()}")
        print(f"åœ°åŒºåˆ†å¸ƒ: {df['æ‰€åœ¨åœ°åŒº'].value_counts().to_dict()}")
        
        return df

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºçˆ¬è™«åŠŸèƒ½"""
    crawler = SupplierCrawler()
    
    # æ›´æ–°ä¾›åº”å•†æ•°æ®åº“
    df = crawler.update_supplier_database()
    
    # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®
    print("\nğŸ“‹ çˆ¬å–æ•°æ®é¢„è§ˆ:")
    print(df.head())
    
    return df

if __name__ == "__main__":
    main()
